---
title: "Sign and zero restrictions: optimism shock on the Australian business cycle"
author: "Xiaolei (Adam) Wang"

execute:
  echo: false

bibliography: references.bib
---

<!-- latex shortcuts -->

\def\*#1{\mathbf{#1}}
\def\e{\boldsymbol{\varepsilon}}

```{r setup, include=FALSE} 
knitr::opts_chunk$set(warning = FALSE, message = FALSE) 
knitr::opts_chunk$set(eval = FALSE)
```

> **Abstract.** This article investigates the effects of optimism shocks on the Australian economy using a Bayesian Structural Vector Autoregression (BSVAR) model. We implement the sign and zero restrictions algorithm proposed by @arias2018inference to identify the optimism shock. Impulse response functions (IRF) and forecast error variance decomposition (FEVD) are used to analyse the effects of the optimism shock on five key macroeconomic variables: productivity, stock prices, consumption, real interest rate and hours worked.
>
> **Keywords.** Bayesian Structural VAR, sign restrictions, zero restrictions, optimism shock

<!-- [Replication Package](https://www.econometricsociety.org/publications/econometrica/2018/03/01/inference-based-structural-vector-autoregressions-identified) -->

# Introduction

**Objective**: The goal of this research project is to implement the sign and zero restrictions algorithm proposed by @arias2018inference, and apply it to analyse the optimism shock in Australia economy.

**Question**: Does optimism shocks drive the business cycles in Australian economy?

**Motivation**: In macroeconomics, there has been a longstanding belief that fluctuations in business cycles can be largely attributed to episodes of optimism and pessimism. An optimism shock is defined as a positive shock to stock prices that does not affect productivity. Using a penalty function approach (PFA), @beaudry2011mood finds significant evidence that optimism shocks are a key driver of business cycles in the United States, while @arias2018inference argue that PFA imposes additional restrictions and they find less significant results using a importance sampler algorithm. This research project aims to extend the analysis to the Australian economy by implementing the importance sampler algorithm, and compare the results with United-States data.

# Data

<!-- load data -->

```{r include=FALSE}
library(readrba)
library(readabs)
library(tidyquant)
library(tseries)
library(tidyverse)
library(knitr)

devtools::install_github("bsvars/bsvarSIGNs")
library(bsvarSIGNs)

consumer_price_index = read_rba(series_id = 'GCPIAG')

productivity         = read_rba(series_id = 'GNFPROSQI')

asx200               = tq_get('^AXJO', from = '1990-01-01', to = '2023-12-31')
# stock_prices       = asx200 / consumer_price_index

retail_turnover      = read_abs(series_id = 'A3348585R')
# consumption        = retail_turnover / consumer_price_index

cash_rate            = read_rba(series_id = 'FIRMMCRI')
inflation            = read_rba(series_id = 'GCPIAGSAQP')
# real_interest_rate = cash_rate - inflation

hours_worked         = read_abs(series_id = 'A85389461V')
```

<!-- concat datasets -->

```{r include=FALSE}
df_consumer_price_index = consumer_price_index |> 
  select(date, value) |>
  rename(consumer_price_index = value) |> 
  mutate(date = as.yearmon(date))

df_productivity = productivity |> 
  select(date, value) |>
  rename(productivity = value) |> 
  mutate(date = as.yearmon(date))

df_asx200 = asx200 |> 
  rename(asx200 = close) |> 
  na.locf(fromLast = TRUE) |> 
  mutate(ym = as.yearmon(date)) |> 
  slice_max(date, by = ym) |> 
  select(date, asx200) |> 
  mutate(date = as.yearmon(date))

df_retail_turnover = retail_turnover |>
  select(date, value) |>
  rename(retail_turnover = value) |> 
  mutate(date = as.yearmon(date))

df_cash_rate = cash_rate |>
  select(date, value) |>
  rename(cash_rate = value) |> 
  mutate(date = as.yearmon(date))

df_inflation = inflation |>
  select(date, value) |>
  rename(inflation = value) |> 
  mutate(date = as.yearmon(date))

df_hours_worked = hours_worked |>
  select(date, value) |>
  rename(hours_worked = value) |> 
  mutate(date = as.yearmon(date))

df = merge(df_consumer_price_index, df_productivity, by = 'date') |>
  merge(df_asx200, by = 'date') |>
  merge(df_retail_turnover, by = 'date') |>
  merge(df_cash_rate, by = 'date') |>
  merge(df_inflation, by = 'date') |>
  merge(df_hours_worked, by = 'date') |> 
  mutate(productivity       = log(productivity),
         stock_prices       = log(asx200 / consumer_price_index),
         consumption        = log(retail_turnover / consumer_price_index),
         real_interest_rate = cash_rate - inflation,
         hours_worked       = log(hours_worked)
         ) |> 
  select(date, productivity, stock_prices, consumption, real_interest_rate, hours_worked)
```

All data are collected from the Reserve Bank of Australia (RBA), the Australian Bureau of Statistics (ABS) and Yahoo Finance. Following @beaudry2011mood, we select the following five variables for our analysis

**Productivity**: non-farm labour productivity per hour (source: RBA, series ID GNFPROSQI).

**Stock prices**: end-of-period ASX 200 index (source: Yahoo Finance, ticker symbol \^AXJO), divided by the consumer price index.

**Consumption**: retail turnover (source: ABS, series ID A3348585R), divided by the consumer price index.

**Real interest rate**: over-night cash rate nets inflation (source: RBA, series ID FIRMMCRI and GCPIAGSAQP).

**Hours worked**: total hours worked (source: ABS, series ID A85389611R).

The first two variables (productivity and stock prices) are chosen to identify the optimism shock, the last three variables (consumption, real interest rate and hours worked) are chosen to capture the business cycle dynamics as in standard macroeconomic theory.

To capture multiplicative relationships in macroeconomic time series and percentage change interpretation, all variables are log transformed (except for real interest rate). A preview of first 6 rows of the concatenated dataset is shown below.

```{r}
kable(head(df), digits = 4)
```

## Time series plot

```{r}
library(ggplot2)

df |> 
  pivot_longer(cols = -date, names_to = 'variable', values_to = 'value') |> 
  ggplot(aes(x = date, y = value, color = variable)) +
  geom_line() +
  theme_bw()
```

The sample period covers 1994Q3 to 2023Q4.

## ACF and PACF plot

```{r}
Y = df |> 
  select(-date) |> 
  ts(start = c(year(min(df$date)), quarter(min(df$date))), frequency = 4)

N = ncol(Y)

par(mfrow = c(2, N - 2))
for (i in 1:ncol(Y)) {
  acf(Y[, i], main = colnames(Y)[i])
}
```

The autocorrelation function (ACF) plot shows all variables have a consistent pattern of autocorrelation, this suggests that the time series are non-stationary. Stationarity is formally tested using the Augmented Dickey-Fuller test in the next section.

```{r}
par(mfrow = c(2, N - 2))
for (i in 1:ncol(Y)) {
  pacf(Y[, i], main = colnames(Y)[i])
}
```

The partial autocorrelation function (PACF) plot shows that the partial autocorrelation of all variables is significant at lag 1, real interest rate is also significant at lag 2. Therefore, choosing a lag length for the VAR model greater than or equal to 2 is reasonable, following convention for quarterly data, we will adopt a lag length of 4 for the VAR model.

## Augmented Dickey-Fuller test

### Level

All five variables are non-stationary at 5% significance level base on the Augmented Dickey-Fuller test.

```{r}
p_value   = sapply(1:N, \(i) adf.test(Y[, i])$p.value)
variable  = colnames(Y)

adf       = cbind(variable, p_value) |> 
  data.frame() |> 
  mutate(p_value = round(as.numeric(p_value), 4)) |> 
  mutate(non_stationary = as.numeric(p_value > 0.05))

kable(adf, digits = 4)
```

### First difference

Applying Augmented Dickey-Fuller test to the first difference of the variables, we find that all variables are stationary at 5% significance level. Therefore, all variables are integrated of order one $I(1)$ and it is reasonable to put them in a VAR system without further transformation.

```{r}
Y_diff    = diff(Y)
p_value   = sapply(1:N, \(i) adf.test(Y_diff[, i])$p.value)
variable  = colnames(Y)

cbind(variable, p_value) |> 
  data.frame() |> 
  mutate(p_value = round(as.numeric(p_value), 4)) |> 
  mutate(non_stationary = as.numeric(p_value > 0.05)) |> 
  kable(digits = 4)
```

# Model

## Specification

Adopting notations from @rubio2010structural, the SVAR model is specified as follows.

### Structural form

$$
\begin{align*}
\*y_t' \*A_0 &= \sum_{l=1}^{p} \*y_{t-l}'\*A_l + \*c + \e_t' \\
\e_t | \*Y_{t-1} &\overset{\text{iid}}{\sim} \mathcal{N}_N(\*0, \*I)
\end{align*}
$$

where $\*y_t$ is an $N\times1$ vector of endogenous variables, $\e_t$ is an $N\times1$ vector of exogenous structural shocks, $\*A_l$ is an $N\times N$ matrix of parameters with $\*A_0$ invertible, $\*c$ is an $1\times N$ vector of parameters, and $p$ is the lag length, and $T$ is the sample size. This can be compactly written as

$$
\begin{align*}
\*y_t' \*A_0 &= \*x_t' \*A_+ + \e_t'
\end{align*}
$$

where $\*A_+ = [\*A_1'\ \cdots\ \*A_p'\ \*c']$ and $\*x_t = [\*y_{t-1}'\ \cdots\ \*y_{t-p}'\ 1]$. The dimension of $\*A_+$ is $K\times N$ where $K=Np+1$.

In matrix form,

$$
\begin{align*}
\*Y \*A_0 &= \*X \*A_+ + \e \\
\e | \* X &\sim \mathcal{MN}_{T\times N}(\*0, \*I_N, \*I_T)
\end{align*}
$$

where $\*Y = [\*y_1\ \cdots\ \*y_T]'$,\ $\*X = [\*x_1\ \cdots\ \*x_T]'$, and $\e = [\e_1\ \cdots\ \e_T]'$.

The matrices $\*A_0$ and $\*A_+$ are structural parameters.

### Reduced form

$$
\begin{align*}
\*y_t' &= \*x_t' \*B + \*u_t' \\
\*u_t | \*Y_{t-1} &\overset{\text{iid}}{\sim} \mathcal{N}_N(\*0, \*\Sigma)
\end{align*}
$$

where $\*B = \*A_ + \*A_0^{-1},\ \*u_t' = \e_t' \*A_0^{-1}$, and

$$
\*\Sigma = \mathbb{E}[\*u_t\*u_t'] = (\*A_0^{-1})' (\*A_0^{-1}) = (\*A_0 \*A_0')^{-1}
$$

In matrix form,

$$
\begin{align*}
\*Y &= \*X \*B + \*u \\
\*u | \* X &\sim \mathcal{MN}_{T\times n}(\*0, \*\Sigma, \*I_T)
\end{align*}
$$

where $\*u = [\*u_1\ \cdots\ \*u_T]'$.

The matrices $\*B$ and $\*\Sigma$ are reduced-form parameters.

### Notation relations

In matrix form, notations from the lectures are

$$
\begin{align*}
\*Y &= \*X A + E \\
\*Y B_0' &= \*X B_+' + U \\
E | \*X &\sim \mathcal{MN}_{T\times N}(\*0, \*\Sigma, \*I_T) \\
U | \*X &\sim \mathcal{MN}_{T\times N}(\*0, \*I_N, \*I_T) \\
\end{align*}
$$

Their equivalence relation is summarized in the following table.

| @rubio2010structural | Lecture     |
|----------------------|-------------|
| $\*B$                | $A$         |
| $\*\Sigma$           | $\*\Sigma$  |
| $\*A_0$              | $B_0'$      |
| $\*A_+$              | $B_+'$      |
| $\e$                 | $U$         |
| $\*u$                | $E$         |

($B_0=B$ in @wozniakBsvarsBayesianEstimation2022)

In the following, we will use the first set of notations.

### Orthogonal reduced-form parameterization

Since SVAR model are identified up to a rotation matrix $\*Q$, we can explicitly specified the reduced-form model as

$$
\*y_t' = \*x_t' \*B + \e_t' \*Q' h(\*\Sigma)
$$

Where $\*Q'h(\*\Sigma) = \*A_0^{-1}$ or $\*Q=h(\*\Sigma) \*A_0$, and $h$ is some differentiable decomposition, one specific choice is the upper triangular Cholesky decomposition. 

We can define a mapping $f_h$ between the reduced-form parameters $(\*B, \*\Sigma, \*Q)$ and structural-form parameters $(\*A_0, \*A_+)$ as

$$
\begin{align*}
f_h(\*A_0, \*A_+) &= (
  \underbrace{\*A_+ \*A_0^{-1}}_\*B,
  \underbrace{(\*A_0 \*A_0')^{-1}}_{\*\Sigma},
  \underbrace{h((\*A_0 \*A_0')^{-1}) \*A_0}_\*Q
  ) \\
f_h^{-1}(\*B, \*\Sigma, \*Q) &= (
  \underbrace{h(\*\Sigma)^{-1} \*Q}_{\*A_0},
  \underbrace{\*B h(\*\Sigma)^{-1} \*Q}_{\*A_+}
  )
  )
\end{align*}
$$

## Algorithm description

The goal is to sample structural parameters ($\*A_0$, $\*A_+$) satisfying **both** the sign and zero restrictions, but the set of structural parameters satisfying the zero restrictions is of Lebesgue measure zero in the set of all structural parameters ($\mathcal{P}(X=x)=0$ for continuous $X$). Luckily, we can sample the set of structural parameters satisfying the sign restrictions conditional on satisfying the zero restrictions.

Here is a high level outline of the algorithm:

1. Sample reduced-form parameters ($\*B$, $\*\Sigma$, $\*Q$) conditional on the zero restrictions.
2. Set $(\*A_0, \*A_+) = f_h^{-1}(\*B, \*\Sigma, \*Q)$.
3. If the sign restrictions are satisfied, keep $(\*A_0, \*A_+)$ and compute a importance weight, otherwise discard.
4. Repeat steps 1-3 until the desired number of samples is obtained.
5. Resample with replacement using the importance weights.

The importance sampling step 5 is needed to manipulate the density induced by step 1 to the desired conjugate posterior density, if one is willing to accept the former density then step 5 is not needed but there are still other drawbacks [@arias2018inference, 4.4].

## Identification

After estimating the reduced-form VAR model, we plan to impose the following restrictions on the impulse response matrix $B$ to identify optimism shock.

| Productivity | Stock prices | Consumption | Real interest rate | Hours worked |
|--------------|--------------|-------------|--------------------|--------------|
| 0            | Positive     | Unrestricted| Unrestricted       | Unrestricted |

The identification strategy is based on the assumption that the optimism shock positively affects stock prices, and has no contemporaneous effect on productivity.

## Interpretation

The impulse response function (IRF) of the SVAR model is used to interpret the effect of the optimism shock on the endogenous variables. Specifically, we are interested in whether a positive optimism shock leads to a simultaneous boom in consumption and hours worked (as in the United States).

The forecast error variance decomposition (FEVD) is used to quantify the relative importance of the optimism shock in explaining the variability of a $h$-step ahead forecast of a particular variable. For example, we will examine the proportion of the variability of consumption and hours worked explained by the optimism shock.












