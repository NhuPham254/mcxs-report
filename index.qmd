---
title: "Sign and zero restrictions: optimism shock on the Australian business cycle"
author: "Xiaolei (Adam) Wang"

execute:
  echo: false

bibliography: references.bib
---

```{r setup, include=FALSE} 
knitr::opts_chunk$set(warning = FALSE, message = FALSE) 
# knitr::opts_chunk$set(eval = FALSE)
```

> **Abstract.** This article investigates the effects of optimism shocks on the Australian economy using a Bayesian Structural Vector Autoregression (BSVAR) model. We implement the sign and zero restrictions algorithm proposed by [@arias2018inference] to identify the optimism shock. Impulse response functions (IRF) and forecast error variance decomposition (FEVD) are used to analyse the effects of the optimism shock on five key macroeconomic variables: productivity, stock prices, consumption, real interest rate and hours worked.
>
> **Keywords.** Bayesian Structural VAR, sign restrictions, zero restrictions, optimism shock

<!-- [Replication Package](https://www.econometricsociety.org/publications/econometrica/2018/03/01/inference-based-structural-vector-autoregressions-identified) -->

# Introduction

**Objective**: The goal of this research project is to implement the sign and zero restrictions algorithm proposed by [@arias2018inference], and apply it to analyse the optimism shock in Australia economy.

**Question**: Does optimism shocks drive the business cycles in Australian economy?

**Motivation**: In macroeconomics, there has been a longstanding belief that fluctuations in business cycles can be largely attributed to episodes of optimism and pessimism. An optimism shock is defined as a positive shock to stock prices that does not affect productivity. Using a penalty function approach (PFA), [@beaudry2011mood] finds significant evidence that optimism shocks are a key driver of business cycles in the United States, while [@arias2018inference] argue that PFA imposes additional restrictions and they find less significant results using a importance sampler algorithm. This research project aims to extend the analysis to the Australian economy by implementing the importance sampler algorithm, and compare the results with United-States data.

# Data

<!-- load data -->

```{r include=FALSE}
library(readrba)
library(readabs)
library(tidyquant)
library(tseries)
library(tidyverse)
library(knitr)

devtools::install_github("bsvars/bsvarSIGNs")
library(bsvarSIGNs)

consumer_price_index = read_rba(series_id = 'GCPIAG')

productivity         = read_rba(series_id = 'GNFPROSQI')

asx200               = tq_get('^AXJO', from = '1990-01-01', to = '2023-12-31')
# stock_prices       = asx200 / consumer_price_index

retail_turnover      = read_abs(series_id = 'A3348585R')
# consumption        = retail_turnover / consumer_price_index

cash_rate            = read_rba(series_id = 'FIRMMCRI')
inflation            = read_rba(series_id = 'GCPIAGSAQP')
# real_interest_rate = cash_rate - inflation

hours_worked         = read_abs(series_id = 'A85389461V')
```

<!-- concat datasets -->

```{r include=FALSE}
df_consumer_price_index = consumer_price_index |> 
  select(date, value) |>
  rename(consumer_price_index = value) |> 
  mutate(date = as.yearmon(date))

df_productivity = productivity |> 
  select(date, value) |>
  rename(productivity = value) |> 
  mutate(date = as.yearmon(date))

df_asx200 = asx200 |> 
  rename(asx200 = close) |> 
  na.locf(fromLast = TRUE) |> 
  mutate(ym = as.yearmon(date)) |> 
  slice_max(date, by = ym) |> 
  select(date, asx200) |> 
  mutate(date = as.yearmon(date))

df_retail_turnover = retail_turnover |>
  select(date, value) |>
  rename(retail_turnover = value) |> 
  mutate(date = as.yearmon(date))

df_cash_rate = cash_rate |>
  select(date, value) |>
  rename(cash_rate = value) |> 
  mutate(date = as.yearmon(date))

df_inflation = inflation |>
  select(date, value) |>
  rename(inflation = value) |> 
  mutate(date = as.yearmon(date))

df_hours_worked = hours_worked |>
  select(date, value) |>
  rename(hours_worked = value) |> 
  mutate(date = as.yearmon(date))

df = merge(df_consumer_price_index, df_productivity, by = 'date') |>
  merge(df_asx200, by = 'date') |>
  merge(df_retail_turnover, by = 'date') |>
  merge(df_cash_rate, by = 'date') |>
  merge(df_inflation, by = 'date') |>
  merge(df_hours_worked, by = 'date') |> 
  mutate(productivity       = log(productivity),
         stock_prices       = log(asx200 / consumer_price_index),
         consumption        = log(retail_turnover / consumer_price_index),
         real_interest_rate = cash_rate - inflation,
         hours_worked       = log(hours_worked)
         ) |> 
  select(date, productivity, stock_prices, consumption, real_interest_rate, hours_worked)
```

All data are collected from the Reserve Bank of Australia (RBA), the Australian Bureau of Statistics (ABS) and Yahoo Finance. Following [@beaudry2011mood], we select the following five variables for our analysis

**Productivity**: non-farm labour productivity per hour (source: RBA, series ID GNFPROSQI).

**Stock prices**: end-of-period ASX 200 index (source: Yahoo Finance, ticker symbol \^AXJO), divided by the consumer price index.

**Consumption**: retail turnover (source: ABS, series ID A3348585R), divided by the consumer price index.

**Real interest rate**: over-night cash rate nets inflation (source: RBA, series ID FIRMMCRI and GCPIAGSAQP).

**Hours worked**: total hours worked (source: ABS, series ID A85389611R).

The first two variables (productivity and stock prices) are chosen to identify the optimism shock, the last three variables (consumption, real interest rate and hours worked) are chosen to capture the business cycle dynamics as in standard macroeconomic theory.

To capture multiplicative relationships in macroeconomic time series and percentage change interpretation, all variables are log transformed (except for real interest rate). A preview of first 6 rows of the concatenated dataset is shown below.

```{r}
kable(head(df), digits = 4)
```

## Time series plot

```{r}
library(ggplot2)

df |> 
  pivot_longer(cols = -date, names_to = 'variable', values_to = 'value') |> 
  ggplot(aes(x = date, y = value, color = variable)) +
  geom_line() +
  theme_bw()
```

The sample period covers 1994Q3 to 2023Q4.

## ACF and PACF plot

```{r}
Y = df |> 
  select(-date) |> 
  ts(start = c(year(min(df$date)), quarter(min(df$date))), frequency = 4)

N = ncol(Y)

par(mfrow = c(2, N - 2))
for (i in 1:ncol(Y)) {
  acf(Y[, i], main = colnames(Y)[i])
}
```

The autocorrelation function (ACF) plot shows all variables have a consistent pattern of autocorrelation, this suggests that the time series are non-stationary. Stationarity is formally tested using the Augmented Dickey-Fuller test in the next section.

```{r}
par(mfrow = c(2, N - 2))
for (i in 1:ncol(Y)) {
  pacf(Y[, i], main = colnames(Y)[i])
}
```

The partial autocorrelation function (PACF) plot shows that the partial autocorrelation of all variables is significant at lag 1, real interest rate is also significant at lag 2. Therefore, choosing a lag length for the VAR model greater than or equal to 2 is reasonable, following convention for quarterly data, we will adopt a lag length of 4 for the VAR model.

## Augmented Dickey-Fuller test

All five variables are non-stationary at 5% significance level base on the Augmented Dickey-Fuller test.

```{r}
p_value   = sapply(1:N, \(i) adf.test(Y[, i])$p.value)
variable  = colnames(Y)

adf       = cbind(variable, p_value) |> 
  data.frame() |> 
  mutate(p_value = round(as.numeric(p_value), 4)) |> 
  mutate(non_stationary = as.numeric(p_value > 0.05))

kable(adf, digits = 4)
```

Applying Augmented Dickey-Fuller test to the first difference of the variables, we find that all variables are stationary at 5% significance level. Therefore, all variables are integrated of order one $I(1)$ and it is reasonable to put them in a VAR system without further transformation.

```{r}
Y_diff    = diff(Y)
p_value   = sapply(1:N, \(i) adf.test(Y_diff[, i])$p.value)
variable  = colnames(Y)

cbind(variable, p_value) |> 
  data.frame() |> 
  mutate(p_value = round(as.numeric(p_value), 4)) |> 
  mutate(non_stationary = as.numeric(p_value > 0.05)) |> 
  kable(digits = 4)
```

# Model

## Specification

A Reduced-form Vector Autoregression (VAR) model has the following specification.

$$
\begin{align*}
y_t &= \mu_0 + A_1 y_{t-1} + \cdots + A_p y_{t-p} + \epsilon_t \\
\epsilon | Y_{t-1} &\overset{\text{iid}}{\sim} \mathcal{MN}(\mathbf{0}_N, \mathbf{\Sigma})
\end{align*}
$$

The corresponding Structural Vector Autoregression (SVAR) model has the following specification.

$$
\begin{align*}
y_t &= \mu_0 + A_1 y_{t-1} + \cdots + A_p y_{t-p} + B u_t \\
u_t | Y_{t-1} &\overset{\text{iid}}{\sim} \mathcal{MN}(\mathbf{0}_N, \mathbf{I}_N)
\end{align*}
$$

Where $y_t$ is a $N \times 1$ vector of endogenous variables

$$
y_t = [\text{productivity}_t,\ \text{stock prices}_t,\ \text{consumption}_t,\ \text{real interest rate}_t,\ \text{hours worked}_t]'
$$

$A_i$'s are $N \times N$ coefficient matrices, $\mu_0$ is a $N \times 1$ vector of constants, $\epsilon_t$ is a $N \times 1$ vector of reduced-form forecast errors, $u_t$ is a $N \times 1$ vector of structural shocks, their relation is captured by the $N \times N$ contemporaneous impulse response matrix $B$:

$$
\epsilon_t = B u_t
$$

## Identification

After estimating the reduced-form VAR model, we plan to impose the following restrictions on the impulse response matrix $B$ to identify optimism shock.

| Productivity | Stock prices | Consumption | Real interest rate | Hours worked |
|--------------|--------------|-------------|--------------------|--------------|
| 0            | Positive     | Unrestricted| Unrestricted       | Unrestricted |

The identification strategy is based on the assumption that the optimism shock positively affects stock prices, and has no contemporaneous effect on productivity.

## Interpretation

The impulse response function (IRF) of the SVAR model is used to interpret the effect of the optimism shock on the endogenous variables. Specifically, we are interested in whether a positive optimism shock leads to a simultaneous boom in consumption and hours worked (as in the United States).

The forecast error variance decomposition (FEVD) is used to quantify the relative importance of the optimism shock in explaining the variability of a $h$-step ahead forecast of a particular variable. For example, we will examine the proportion of the variability of consumption and hours worked explained by the optimism shock.












